{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb29e76",
   "metadata": {},
   "source": [
    "## Handwritten Digit Recognition using Machine Learning and Deep Learning using MNIST Dataset.\n",
    "\n",
    "In the realm of data science, the challenge of handwritten digit recognition has long persisted, requiring robust machine learning and deep learning models to accurately classify digits from 0 to 9. Leveraging the MNIST dataset, comprising 70,000 images of handwritten digits, with 60,000 for training and 10,000 for testing, we embark on the journey of developing a solution to this task. These grayscale images, each measuring 28x28 pixels, provide a rich yet manageable dataset for our endeavors. Our primary aim is to employ Python, along with libraries like Scikit-Learn and Keras, to construct models capable of accurately discerning handwritten digits, thus presenting a foundational challenge in the realm of computer vision and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621eadd",
   "metadata": {},
   "source": [
    "### Loading and Splitting Data.\n",
    "First, we need to load the dataset from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5ab1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nandi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (60000, 28, 28)\n",
      "Number of training labels: 60000\n",
      "Shape of testing images: (10000, 28, 28)\n",
      "Number of testing labels: 10000\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "#Load the data and split it into train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Shape of training images:\", X_train.shape)\n",
    "print(\"Number of training labels:\", len(y_train))\n",
    "print(\"Shape of testing images:\", X_test.shape)\n",
    "print(\"Number of testing labels:\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7359c27",
   "metadata": {},
   "source": [
    "### Reshaping for Handwritten Digit Recognition\n",
    "Now, we need to reshape the training and testing data into a format suitable for neural network training. By transforming the images into a 28x28 pixel grid with a single channel, we prepare the data to fit into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878ea124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images after reshaping: (60000, 28, 28, 1)\n",
      "Shape of testing images after reshaping: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the data to fit the model\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "\n",
    "# Reshape testing data\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "print(\"Shape of training images after reshaping:\", X_train.shape)\n",
    "print(\"Shape of testing images after reshaping:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b209a2",
   "metadata": {},
   "source": [
    " ### One-Hot Encoding for Digit Labels\n",
    "Now, we need to apply one-hot encoding to the digit labels, converting them into a binary matrix representation. This transformation enables the model to effectively learn and classify digits, improving its performance in handwritten digit recognition tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a341106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one-hot encoded training labels: (60000, 10)\n",
      "Shape of one-hot encoded testing labels: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-Hot Encoding:\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "print(\"Shape of one-hot encoded training labels:\", y_train_one_hot.shape)\n",
    "print(\"Shape of one-hot encoded testing labels:\", y_test_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad5a29",
   "metadata": {},
   "source": [
    "### Building and Training CNN for Digit Recognition\n",
    "Now, we need to design and train a Convolutional Neural Network (CNN), which consists of convolutional and dense layers. Additionally, we'll predict the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a6ec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 34ms/step - accuracy: 0.9039 - loss: 0.7373 - val_accuracy: 0.9790 - val_loss: 0.0714\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 37ms/step - accuracy: 0.9775 - loss: 0.0751 - val_accuracy: 0.9831 - val_loss: 0.0549\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 33ms/step - accuracy: 0.9852 - loss: 0.0500 - val_accuracy: 0.9773 - val_loss: 0.0836\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 33ms/step - accuracy: 0.9872 - loss: 0.0402 - val_accuracy: 0.9773 - val_loss: 0.0827\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 36ms/step - accuracy: 0.9892 - loss: 0.0336 - val_accuracy: 0.9818 - val_loss: 0.0694\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 38ms/step - accuracy: 0.9912 - loss: 0.0255 - val_accuracy: 0.9818 - val_loss: 0.0711\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 33ms/step - accuracy: 0.9937 - loss: 0.0209 - val_accuracy: 0.9782 - val_loss: 0.0872\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 32ms/step - accuracy: 0.9937 - loss: 0.0192 - val_accuracy: 0.9846 - val_loss: 0.0719\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 33ms/step - accuracy: 0.9947 - loss: 0.0169 - val_accuracy: 0.9829 - val_loss: 0.0911\n",
      "Epoch 10/10\n",
      "\u001b[1m1464/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9958 - loss: 0.0130"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_one_hot, validation_data=(X_test, y_test_one_hot), epochs=10)\n",
    "\n",
    "# Load and preprocess the image\n",
    "# image_path = \"./test_img.jpg\"\n",
    "image_path = \"./test_img1.png\"\n",
    "# image_path = \"./test_img2.png\"\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "image = image.resize((28, 28))  # Resize\n",
    "image_array = np.array(image)  # Convert to numpy array\n",
    "image_array = image_array.reshape((1, 28, 28, 1))  # Add batch and channel dimensions\n",
    "\n",
    "# Predict the image\n",
    "prediction = model.predict(image_array)\n",
    "\n",
    "# Convert prediction to a readable label\n",
    "predicted_label = np.argmax(prediction)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"output.txt\"\n",
    "\n",
    "# Export the variable to a text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(str(predicted_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5d139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
